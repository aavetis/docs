---
title: "Fetching Prompts"
description: "Use your managed prompts with the Traceloop SDKs"
---

Make sure you've configured the SDK with the right environment and API Key. See the [SDK documentation](/openllmetry/configuration#api-key) for more information.

<Tip>
  The SDK uses smart caching mechanisms to proide zero latency for fetching prompts.
</Tip>

## Get Prompt API

Let's say you've created a prompt with a key `joke_generator` in the UI and set ot to:

```
Tell me a joke about OpenTelemetry as a {{persona}}
```

Then, you can retrieve it with in your code using `get_prompt`:

```python
from traceloop.sdk.prompts import get_prompt

prompt_args = get_prompt("joke_generator", persona="pirate")
completion = openai.ChatCompletion.create(**prompt_args)
```

<Tip>
  The returned variable `prompt_args` is compatible with the API used by the
  foundation models SDKs (OpenAI, Anthropic, etc.) which means you should directly
  plug in the response to the appropriate API call.
</Tip>
