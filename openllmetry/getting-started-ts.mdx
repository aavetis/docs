---
title: "Node.js"
---

<Warning>
  If you're on Next.js, follow the [Next.js
  guide](/openllmetry/getting-started-nextjs).
</Warning>

Install Traceloop by following these 3 easy steps and get instant monitoring.

<Steps>
<Step title="Install the SDK">

Run the following command in your terminal:

<CodeGroup>
```bash npm
npm install @traceloop/node-server-sdk
```

```bash pnpm
pnpm install @traceloop/node-server-sdk
```

```bash yarn
yarn add @traceloop/node-server-sdk
```

</CodeGroup>

In your LLM app, initialize the Traceloop tracer like this:

```js
import * as traceloop from "@traceloop/node-server-sdk";

traceloop.initialize();
```

<Warning>
  Because of the way Javascript works, you must import the Traceloop SDK before
  importing any LLM module like OpenAI.
</Warning>

If you're running this locally, you may want to disable batch sending, so you can see the traces immediately:

```js
traceloop.initialize({ disableBatch: true });
```

If you are using the prompt registry, then you should wait for the SDK to become ready before fetching any prompts.
`waitForInitialization` will return a promise that resolves when the SDK is ready, or reject on error.

```js
await traceloop.waitForInitialization();
```

</Step>
<Step title="Annotate your workflows">
<Frame>
  <img src="/img/workflow.png" />
</Frame>
If you have complex workflows or chains, you can annotate them to get a better understanding of what's going on.
You'll see the complete trace of your workflow on Traceloop or any other dashboard you're using.

We have a set of [decorators](/openllmetry/tracing/decorators) to make this easier.
Assume you have a function that renders a prompt and calls an LLM, simply add `@workflow` to a method.

<Tip>
  If you're using an LLM framework like Haystack, Langchain or LlamaIndex -
  we'll do that for you. No need to add any annotations to your code.
</Tip>

```js
class MyLLM {
  @workflow("suggest_answers")
  async suggestAnswers(question: string) {
    ...
  }
}
```

For more information, see the [dedicated section in the docs](/openllmetry/tracing/decorators).

</Step>
<Step title="Configure trace exporting">
<Tip>
  If you just want to explore - you don't need to do anything! Just run your app
  without setting any environment variable and a new account will automatically
  be created for you so you can start seeing traces from your development
  environment.
</Tip>
Lastly, you'll need to configure where to export your traces.
The 2 environment variables controlling this are `TRACELOOP_API_KEY` and `TRACELOOP_BASE_URL`.

For Traceloop, read on. For other options, see [Exporting](/openllmetry/integrations/exporting).

### Using Traceloop Cloud

Go to [Traceloop](https://app.traceloop.com), and create a new account.
Then, click on **Environments** on the left-hand navigation bar. Or go to directly to https://app.traceloop.com/settings/api-keys.
Click **Generate API Key** to generate an API key for the development environment and click **Copy API Key** to copy it over.

<Warning>Make sure to copy it as it won't be shown again.</Warning>

<Frame>
  <img src="/img/apikey.png" />
</Frame>

Set the copied Traceloop's API key as an environment variable in your app named `TRACELOOP_API_KEY`.

Done! You'll get instant visibility into everything that's happening with your LLM.
If you're calling a vector DB, or any other external service or database, you'll also see it in the Traceloop dashboard.

</Step>
</Steps>
